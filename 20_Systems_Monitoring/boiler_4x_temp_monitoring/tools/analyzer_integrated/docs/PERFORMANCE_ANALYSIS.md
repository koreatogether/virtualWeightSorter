# 데이터 분석기 성능 저하 원인 분석 보고서

## 1. 현상
* `docs/logs/YYYY-MM-DD` 폴더 내에 약 18,000개 이상의 JSON 파일이 존재할 경우, 웹 대시보드 로딩 및 차트 렌더링 속도가 급격히 저하됨.

## 2. 주요 원인 분석

### 2.1 서버 측 I/O 병목 (가장 심각)
* **파일 오픈 오버헤드**: 현재 서버(`analyzer_server.py`)는 특정 날짜 조회 시 해당 폴더의 **모든 JSON 파일을 개별적으로 오픈** (`open()`) 합니다. 18,000번의 시스템 콜은 OS와 하드웨어 레벨에서 매우 큰 부하를 발생시킵니다.
* **반복적인 JSON 파싱**: 파일을 하나 열 때마다 `json.load()`를 수행하므로, CPU가 텍스트를 객체로 변환하는 작업을 수만 번 반복해야 합니다.

### 2.2 네트워크 전송 부하
* **거대 데이터 전송**: 수만 개의 데이터 포인트를 하나의 대형 JSON 배열로 묶어 브라우저로 전송합니다. 데이터 크기가 수 MB에서 수십 MB에 달할 수 있어 다운로드 시간이 길어집니다.

### 2.3 브라우저 렌더링 부하
* **Chart.js 한계**: 브라우저의 `Chart.js` 라이브러리가 수만 개의 점(Point)을 한 화면에 선으로 그리려 할 때, 자바스크립트 엔진과 GPU에 과부하가 걸려 화면 스크롤이나 줌(Zoom) 동작이 멈추는 현상이 발생합니다.

## 3. 개선 방안

### 3.1 저장 방식 변경 (Short-term)
* **Single File Logging**: 매번 파일을 새로 만드는 대신, 하루치 데이터를 하나의 파일(예: `2026-02-07.log`)에 한 줄씩 추가(Append)하는 방식으로 변경합니다. 서버는 파일 하나만 읽으면 되므로 속도가 수백 배 향상됩니다.
* **SQLite 도입**: 파일 대신 경량 DB를 사용하여 인덱싱을 처리하면 훨씬 더 정교한 시간대별 조회가 가능합니다.

### 3.2 데이터 요약 (Downsampling)
* **샘플링 전송**: 2초 간격의 모든 데이터를 보낼 필요가 없습니다. 서버에서 데이터를 읽을 때 10개 혹은 30개당 1개만 추출(Skip)하거나, 1분 단위 평균값을 계산하여 브라우저에 전달합니다.

### 3.3 서버 캐싱
* **통합 캐시 생성**: 폴더 내 개별 JSON들을 읽어 한 번 처리했다면, 결과물을 `all_data.cache.json` 같은 파일로 저장해두고 다음 요청 시에는 이 파일만 읽도록 합니다.

---
**작성일**: 2026-02-08
**분석 도구**: Boiler Integrated Analyzer v2.0
